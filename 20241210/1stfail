from bs4 import BeautifulSoup

def get_sources_10k(file_path):
    """
    Extract Item1, Item1a, Item7, and the income statement from the 10-K document.

    Args:
        file_path (str): Path to the HTML file.

    Returns:
        dict: Dictionary containing extracted sections.
    """
    # Load and parse the HTML file
    with open(file_path, 'r', encoding='utf-8') as file:
        soup = BeautifulSoup(file, 'lxml')

    # Define search keys and outputs
    sections = {
        "item1": "Item 1",
        "item1a": "Item 1A",
        "item7": "Item 7",
        "income_statement": "Consolidated Income Statement"
    }
    extracted_data = {}

    # Extract each section based on keywords
    for key, section_name in sections.items():
        content = []
        for tag in soup.find_all(text=lambda text: text and section_name.lower() in text.lower()):
            parent = tag.find_parent()  # Find the parent node to capture more context
            if parent:
                content.append(parent.get_text(separator=" ", strip=True))

        extracted_data[key] = "\n".join(content) if content else "Section not found."

    return extracted_data

# Test the function with Tesla's FY23 document
tesla_fy23_path = "./tsla-20231231.html"
extracted_sections = get_sources_10k(tesla_fy23_path)
extracted_sections

===================================================================================================================
from bs4 import BeautifulSoup

def extract_section(html_content, item):
    soup = BeautifulSoup(html_content, 'html.parser')

    # Find the header for the desired item (e.g., "ITEM 1")
    target_header = soup.find(string=lambda text: text and item.lower() in text.lower())
    if not target_header:
        return f"Section '{item}' not found."

    # Traverse siblings and collect content until the next "ITEM" header
    extracted_text = []
    current_tag = target_header.find_parent()
    while current_tag:
        current_tag = current_tag.find_next()
        if current_tag and current_tag.name and current_tag.name in ['h1', 'h2', 'span']:
            if 'ITEM' in current_tag.get_text(strip=True).upper():
                break
        if current_tag:
            extracted_text.append(current_tag.get_text(strip=True))

    return ' '.join(extracted_text).strip()

# Example usage
html_file_path = '/content/tsla-20231231.html'  # Replace with your actual file path

with open(html_file_path, 'r', encoding='utf-8') as file:
    html_content = file.read()

item_to_extract = 'ITEM 1'  # Replace with the desired section (e.g., 'ITEM 2')
extracted_section = extract_section(html_content, item_to_extract)

print(f"Extracted content for {item_to_extract}:\n{extracted_section}")




============================================================================================================================================================================

from bs4 import BeautifulSoup

def extract_section(html_content, start_item, stop_item):
    soup = BeautifulSoup(html_content, 'html.parser')

    # Find the start of the target item
    start_header = soup.find('span', text=lambda x: x and start_item.lower() in x.lower())
    if not start_header:
        return f"Section '{start_item}' not found."

    # Extract text until the stop item
    extracted_text = []
    current_tag = start_header.find_parent()
    while current_tag:
        current_tag = current_tag.find_next()
        if current_tag and current_tag.name in ['div', 'span', 'p']:
            if stop_item.lower() in current_tag.get_text(strip=True).lower():
                break  # Stop when the stop_item is encountered
            extracted_text.append(current_tag.get_text(strip=True))

    return ' '.join(extracted_text).strip()

# Example usage
html_file_path = '/content/tsla-20231231.html'  # Replace with your actual file path

with open(html_file_path, 'r', encoding='utf-8') as file:
    html_content = file.read()

# Specify the start and stop sections
start_item = 'ITEM 1. BUSINESS'
stop_item = 'ITEM 1A. RISK FACTORS'

# Extract the desired section
extracted_section = extract_section(html_content, start_item, stop_item)

print(f"Extracted content for {start_item}:\n{extracted_section}")


